<!DOCTYPE HTML>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
	<link rel="bookmark" type="image/x-icon" href="/img/logo_miccall.png">
	<link rel="shortcut icon" href="/img/logo_miccall.png">
	
			    <title>
    chenhaoze.com
    </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link rel="stylesheet" href="/css/mic_main.css">
    <link rel="stylesheet" href="/css/dropdownMenu.css">
    <meta name="keywords" content="chenhaoze">
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css">
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/wallpaper3.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
</head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css">
<link rel="stylesheet" href="/css/typo.css">
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">个人主页</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special">
            <ul class="menu links">
			<!-- Homepage  主页  --> 
			<li>
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/DEEP-LEARNING/">DEEP LEARNING</a></li><li><a class="category-link" href="/categories/练习题/">练习题</a></li><li><a class="category-link" href="/categories/论文/">论文</a></li><li><a class="category-link" href="/categories/随笔/">随笔</a>
	                    </li></ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="关于我">
		                关于我
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图集">
		                图集
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
			</ul>
</nav>

        <div id="main">
            <div class="post_page_title_img" style="height: 25rem;background-image: url(https://chen950203.github.io/7.png);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;">
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2>Vehicle Type Identification</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <p><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script><br><strong><em>Abstract：</em></strong><br>Due to the audio information of different types of vehicle models are distinct, vehicle information can be identified by the audio signal of vehicle accurately. In real life, in order to determine the type of vehicle, we do not need to obtain the visual information of vehicles and just need to obtain the audio information. In this paper, we extract the fusion feature from different aspects: Mel frequency cepstrum coefficients in perceptual characteristics, pitch class profile in psychoacoustic characteristics and short-term energy in acoustic characteristics. In addition, we improve the performance of neural network classifier by fusing long short-term memory (LSTM) unit into the convolutional neural networks. At last, we put the fusion feature to the hybrid neural networks to recognize different vehicles. The results suggest that the fusion feature we proposed in this paper can increase the recognition rate by 7%; and LSTM has great advantages in modeling time series, adding LSTM to the networks can improve the recognition rate of 3.39%.</p>
<p><strong>Keywords</strong>—vehicle audio signals, fused feature, LSTM, convolutional neural networks, hybrid neural networks, identification system*</p>
<p><strong>I.INTRODUCTION</strong><br>In modern transportation system, the application of classifying different vehicle types is quite crucial in daily life, such as intelligent monitoring system, auto-charging system and illegal preemption of way detection [1]. But in real life, it is not possible to capture vehicle images clearly due to complex lighting and harsh weather. The effect of vehicle identification by acquiring the images is not satisfied.<br>The collection of audio is not affected by harsh weather, such as fog and heavy rain. Identification system using vehicle audio is very robust. And it is more convenient than the image collection. Wu showed that support vector machine is suitable for classification [2]. Zhao H.X. [3] identified the vehicle type using feature fusion. Inagaki k et al. showed that using Recurrent Neural Network (RNN) can classify vehicle types accurately [4]. Zhou studied how to extract vibration signal feature to classify different vehicles [5]. Fan of Tsinghua University studied composed wavelet transform and mutual information to recognize different types of vehicles [6]. The vehicle type identification based on audio signals divide into two processes: feature extraction and the identification. Feature extraction is the process that extracting more comprehensive information from vehicle audio signals; after extracting, a suitable classifier is designed to identify different vehicles.</p>
<p><strong>II.EXTRACTING VEHICLE AUDIO FEATURES</strong><br>Acoustic characteristics, perceptual characteristics and psychoacoustic characteristics make up audio characteristics [7]. Time and frequency domain parameters constitute acoustic characteristics, including the short-time energy, the short-time autocorrelation function, and the short-time zero crossing rate and so on. Perceptual characteristics are the parameters extracted from human auditory, including the Mel Frequency Cepstrum Coefficients, and its first-order and second-order differential which are used to reflect the dynamic characteristics. Psychoacoustic characteristics include loudness, roughness and pitch class profile etc.</p>
<p><em>A.Mel Frequency Cepstrum Coefficient (MFCC)</em><br>MFCC was first used in the field of automatic speech recognition (ASR) and speaker recognition [8]. MFCC is a kind of cepstral coefficient based on Mel frequency. The conversion formula can be expressed as:<br></p>
<center>$Mel(f)=2529lg(1+f/700)\qquad (1)$</center>
Where, $f$ is the frequency.
After the pre-processing of vehicle audio signals, $MFCC$ vector will be extracted from each frame in the form of vector group. Fast Fourier transform (FFT) is applied to each frame of the audio signals before sending to Mel filter where the original spectrum is transformed into Mel frequency spectrum. Then logarithmic transformation and discrete cosine transform are applied to the spectrum to form MFCC vectors. Equation (2) to (5) reflect the entire transformation process. Fig.1. shows the process of extracting MFCC.<br>

<center>$s(k)=\sum_{k=1}^{n}x(m)e^{(-j2/pik/N)},0\leq k\leq N\qquad (2)$</center>

<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/MFCC.jpg" alt><br></div></p>
<font color="black" size="2">Figure 1: The process of extracting MFCC</font>

<p>$x(m)$ is the input audio signal, $N$ is the number of Fourier transform.<br>Then we use $M$ Mel filter banks to filter the audio signal. The transfer function of the mth filter bank can be expressed as follows:<br></p>
<center>
$H_{m}(k)=\begin{cases} 
\qquad \qquad  0 \qquad \qquad \qquad \quad , \quad k< f(m-1) \\\\
\frac{2(k-f(m-1))}{(f(m+1)-f(m-1))(f(m)-f(m-1))},\quad f(m-1)< k< f(m) \\\\
\frac{2(f(m+1)-k)}{(f(m+1)-f(m-1))(f(m+1)-f(m))},\quad f(m)< k< f(m+1) \\\\
\qquad \qquad 0 \qquad  \qquad \qquad \quad , \quad k> f(m+1) 
\end{cases}\qquad (3) $
</center><br>
Where, $\sum_{k=0}^{n}H_{m}(k)=1$, $f(m)$ is the center frequency of the triangular filter. Then the logarithmic transformation of the $m$th filter bank can be expressed as:
<center>$s(m)=ln\left(\sum_{k=0}^{M-1}|s(k)|^{2}H_{m}(k)\right),0\le m\le M, \qquad (4) $</center><br>
Finally, we perform a discrete Fourier transform on a to obtain the Mel frequency cepstrum coefficient:<br>
<center>$C(n)=\sum_{n=0}^{M-1}s(m)cos(\pi n(m+0.5)/M),\qquad (5)$</center>
Where, $M$ is the dimension of the MFCC feature.<br>

B. Pitch Class Profile (PCP)<br>
The pitch class profile can be used to extract the characteristics of the audio tone. It reconstructs the spectrum into sound spectrum, which describing the level of sound 16. When extracting the MFCC, we send the audio signals to Mel filter to smooth spectrum and eliminate harmonics. The formant of audio will be highlighted. So the tone characteristics of vehicle audio will be ignored. But the vehicle audio contains complex audio signals including horn sound, tire friction sound and engine sound, etc. So the pitch class profile can be selected to be the characteristic to recognize different types of the vehicles. The process of extracting PCP is shown in Fig. 2.<br>

<div align="center">![](./Hybrid/1.jpg)<br>

<font color="black" size="2">Figure 2: The process of extracting PCP</font>
The specific extraction steps are as follows:<br>
$Step 1:$  $x(m)$ is the collected audio signal, we segment the pre-processed audio into frames, $N$ is frame length; $N/2$ is frame shifting; we use Hamming window as window function.
<center>$X^{cqt}(k)=\frac{1}{N}\sum_{m=0}^{N_k-1}x(m)w_{N_k}(m)e^-j\frac{2\pi Q}{N_k}m,\qquad (6)$</center>
The formula represents the spectrum of the $k$th semitone of the nth frame.  $w_{N_k}(m)$  is the window and  $N_{k}$ is the length of the window. $Q$ is the constant factor of Constant-Q spectral analysis.<br>
$Step 2:$ A 12-bin tuned Chromagram is then calculated from the Constant-Q spectra. The CQT spectrum is mapped to a pitch contour feature in a logarithmic manner according to the averaging law in music theory. The mapping formula is:<br>
<center>$p(k)=[12\times ln(k\cdot f_{s}/N\cdot f_{ref})]mod12 \qquad  (7)$</center>
In the formula, $f_s$ is sampling rate; $f_{s}/N$ is the frequency domain interval; $f_{ref}$ is the reference frequency, corresponding to the $C1$ in music; indicates the frequency of each frequency domain.
$Step 3:$ Accumulate all the frequency which corresponding to the pitch class, we will obtain the 12-bin PCP vector:
<center>$PCP(p)=\sum_{k:p(k)=p}|X^{cqt}(k)|,p=1,2...12, \qquad  (8)$</center>
$Step 4:$ In order to make the $PCP$ more dynamic, we perform a first-order difference on the $PCP(p)$ to get $PCP'(p)$ and then stitch $PCP(p)$ and $PCP'(p)$.

C. Short-term Energy<br>
The energy can reflect vehicle audio feature more comprehensively.  $x_{n}(m)$ is the $n$th frame of pre-processed audio signal. $E_{n}$ is the short-term energy:
<center>$E_{n}=\sum^{L}_{m=1}[x_{n}(m)]^{2}\qquad (9)$</center>
In the formula, $L$ is the length of the frame.
We extract 40 dimensional MFCC vector $T_{1}=[a_{1},a_{2},...,a_{40}]$, 24 dimensional PCP vector $T_{2}=[b_{1},b_{2},...,b_{24}]$, and 1 dimensional short-term energy vector $T_{3}=[c_{1}]$, and stitch from beginning to end, obtaining 65 dimensional new feature $T_{1}=[a_{1},...,a_{40},b_{1},b_{2},...,b_{24},c_{1}]$.

**III.Hybrid neural network**
A Convolutional Neural Network (CNN)
Essentially, CNN is an extended structure of multi-layer perceptron, the construction of the network layer structure has a great impact on the actual output. The classic convolutional neural networks include input layer, convolutional and pooling layer combined in various forms, a limited number of fully connected layer, and output layer. In the convolutional layer, the convolution kernel performs a convolution operation on the vector which export from the previous layer. Nonlinear system can be obtained by activation function, mathematical model can be expressed as:
<center>$x^{l}_{j}=f(\sum_{i \in M_{j}}x^{i}_{j} \times k^{l}_{ij}+b^{l}_{j})\qquad (10)$</center>

<p>In the formula, $M<em>{j}$ is input vector. $l$ is the layer $l$ network; $k$ is the convolution kernel; $b$ is bias; $x^{i}</em>{j}$ is the output of layer $l$. We select $ReLU$ as the activation function.</p>
<p>B Long Short-Term Memory (LSTM)<br>Long short-term memory (LSTM) is a special kind of RNN [17], mainly to solve the problem of gradient disappearance and gradient explosion during long sequence training. To put it simply, LSTM can perform better in longer sequences than ordinary RNN. A LSTM model includes the input gate it, the output gate $o_t$, the forget gate $f_t$ and a cell $c_t$. We can see the LSTM structure from Fig.3.<br></p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/3.png" alt><br></div></p>
<p>Figure3: Structure of the LSTM unit.<br>The forget gate can be express as:</p>
<center>$f_t=\sigma(W_f[x_t,h_t, C_{t-1}]+b_t)*C_{t-1}\qquad (11)$</center>

<p>Where, $x<em>{t}$ is input unit; $h</em>{t-1}$ is the output unit; $C_{t-1}$ is the status of previous unit; $W_f$ is weight; bt is the bias; σ is the sigmoid. Input gate and output gate can be expressed by formula (15)-(18):</p>
<center>$i_t=\sigma(W_i[x_t,h_{t-1}, C_{t-1}]+b_i)\qquad (12)$</center>

<center>$C_t=f_t+i_t*tanh(W_i[x_t,h_{t-1}, C_{t-1}]+b_i)\qquad (13)$</center>

<center>$o_t=\sigma(W_o[x_t,h_{t-1}, C_t]+b_o)\qquad (14)$</center>

<center>$h_t=tanh(C_t)*o_t\qquad (15)$</center>

<p>In the formula, $C_{t-1}$ is the output status of the input gate; $h_t$ is the output of the LSTM unit. The LSTM is fed by the softmax output of the CNN as the feature.<br>The hybrid neural network classifier proposed in this paper is shown in Fig. 4. It includes 1 input layer, 3 convolution layers, 2 Batch Normalization layers, 2 pooling layers. Conv1d represents 1-dimensional convolution and LSTM adopts one time step.</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/4.png" alt><br></div></p>
<p>Figure 4: Structure of the hybrid neural network classifier.</p>
<p><strong>IV.Experimental Results</strong><br>A Effectiveness of novel feature<br>To verify the validity of the novel feature which proposed in this paper with support vector machine (SVM) and hybrid neural network classifiers (HNN), we use the data of an extensive real world experiment (<a href="http://www.ecs.umass.edu/~mduarte/Software.html" target="_blank" rel="noopener">http://www.ecs.umass.edu/~mduarte/Software.html</a>), which includes vehicle sensor signals for both assault amphibian vehicle and Dragon Wagon. This paper adopts 180 vehicle signal data collected from the same sampling location. In this experiment, we divided the dataset into training set and test set. Due to the small amount of experimental data, and in order to utilize as much as possible the limited number of datasets, we used a 10-fold cross-validation method to Evaluate all methods where 90% of the data was selected for training ,10% for testing and for 10 non-overlap test dataset repeat 10 times. It makes all samples available for training the model. Firstly, we preprocessed these data, the process is as follows: the length of the frame and window are both 128, the frame shifting is 64, resampling to 44.1 kHz. We spliced two of the three features and compared with the novel features proposed in this paper in order to verify the effectiveness of the novel feature proposed in this paper. To avoid the contingency of the experiment, we use multiple experiments and take the average as the experimental result.</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/5.png" alt><br></div></p>
<p>Figure 5: The accuracy of different feature experiment<br>The experimental results are shown in Fig. 5. As we can see from the result that the novel feature can capture audio information from different aspects, retain information from the psychoacoustic feature, the acoustic feature and the perceptual feature, which can represent the vehicle audio information more comprehensively and make the recognition more accurately. It can also be seen from the figure that compared with other features, the method combining these three feature vectors can improve the recognition accuracy by nearly 7%. In the meantime, the novel feature is unidimensional, so the complexity of the algorithm has not increased. The feature containing MFCC has a better recognition effect, since it is a good simulation of human auditory sensation. As shown in Figure 5, when using the 10-fold crossover method, the average accuracy of the novel features as input features can reach 100%, which can fully prove the superiority of the novel features extracted in this paper, and the recognition rate is higher than other features. On the other hand, in different numbers of test sets, the recognition accuracy with HNN which proposed in this paper is higher than SVM.<br>B Verification experiment by augmenting dataset<br>In order to verify the generalization ability of the vehicle recognition system, the city noise is superimposed on the vehicle audio signal, and the signal noise ratio (SNR) is 20 dB, 15 dB, 10 dB, 5 dB and 0 dB respectively. We randomly select 80% of the data sets to add noise with different signal-to-noise ratios, so that on the one hand, the size of data sets can be expanded, and on the other hand, each vehicle can be simulated in different environments. After training, we tested the recognition accuracy of the model using the test set. Here, the test set can be divided into two parts, one part is the audio with noise added, and the other part is the original data set without noise. We use the same data as previous experiment under the same conditions. We select the novel feature as the input vector. We also use the 10-fold cross-validation method described in Section 4.1 to divide the data set. Dropout rate is set to 0.25. The initial learning rate is set to 0.001. Moving average attenuation coefficient is set to 0.9. Stochastic gradient descent algorithm (SGD) is used to update the learning rate. In order to reflect the performance of dataset augmentation more visually, we trained the original signal without superimposing noise under the same conditions and tested with the noisy signal. We performed five experiments for each SNR signal to guarantee the reliability. The training set accounted for 70% of the total data set. The experimental recognition accuracy is shown in Fig. 6.<br>As shown in Fig. 6., when experiment with dataset augmentation and the SNR is 0 dB, the recognition accuracy is 98.46%. However, the accuracy of the experiment without dataset augmentation is 94.5%. When the signal to noise ratio is in the range of 0 dB to 20 dB, the stability and accuracy of the model with dataset augmentation are significantly higher than the latter. The anti-interference ability of the model is greatly improved. Furthermore, it is verified that this recognition system has high accuracy in different SNR.</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/6a.png" alt><br></div></p>
<p>(a) Results of the test set without superimposed noise</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/6b.png" alt><br></div></p>
<p>(b) Results of the test set with superimposed noise<br>Figure 6: Comparative experiments</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/7.png" alt><br></div></p>
<p>Figure 7: The process of audio recording<br>C Verification experiment of sensor acquisition data<br>This experiment aims to verify the robust of the recognition system, we recorded 10 different kinds of vehicle audio, including YUTONG bus, JAC truck, HAVAL H5, SGMW, RANGE ROVER, JETTA, HYUNDAI IX35, BYD e6, JAC V7, and train. The recording process of the audio is shown in Fig. 7. The collected equipment includes a microphone, a PXI Vector Signal Transceivers and a computer. The number of each vehicle sample is 100. The sampling frequency is 44.1kHz. The audio contains the vehicle’s engine sound, horn sound, brake sound and the tire friction sound. Due to the interference noise from different vehicles when collecting, we intercepted 132,300 sample points containing the largest ingredient of the main vehicles as the sample point data. The collected vehicle audio waveform is shown in Fig. 8. To verify the impact of the LSTM layer on the recognition accuracy of the system, under the same experimental condition, we randomly selected 80% of the training set to superimpose noise.</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/8.png" alt><br></div></p>
<p>Figure 8: The vehicle audio waveform.<br>We experimented with LSTM layer and without LSTM layer separately and performed 5 experiments according to the test set ratio of 20%, 25%, 30%, 35%, 40% and 45% respectively. The accuracy and recall of experiments are shown in Table 1.<br>Table 1. The accuracy and recall of the experiment</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/9.png" alt><br><br>As shown in Table 1, adding LSTM layer to the networks can improve the recognition rate of 3.39%. The reason for this effect is that the LSTM unit has a certain memory for the front and back frame information of the audio, so the correlation between the frictional sound in the signal and the engine sound appears obviously. The LSTM unit has great advantages in modelling time series. Therefore, adding the LSTM layer to the neural network classifier has a great effect on improving the system performance.<br>Fig. 9 is the confusion matrix of the experiment. We selected 700 of them as training sets. It can be seen that the recognition rate of large vehicles is very high, the reasons are that the large vehicles are heavy and the friction between tires and ground is more characteristic than the other small vehicles. Furthermore, large vehicles use diesel engines, the distinctive noise is caused largely by the way the fuel ignites. BYD e6 is the new energy vehicle, its motor drive will not have the same vibration and noise as the internal combustion engine, so the characteristics are relatively obvious compared with gasoline and diesel engines, so it has a good recognition effect. Fig. 10 shows the accuracy and loss function of the experiment. We can see that the recognition accuracy tends to be stable after 150 iterations, and has remained at around 98%. The loss function tends to be stable about 200 iterations, and the test set loss function curve has a high similarity with the training set, tending to 0.05, indicating that the model has strong robustness.</div></p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/10.png" alt><br></div></p>
<p>Figure 9: The confusion matrix of the experiment</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/11.png" alt><br></div></p>
<p>Figure 10: The accuracy and loss of the experiment<br>To verify the impact of different types of noise, we randomly selected 80% of the audio superimposing the wind noise, engine noise, rain noise, thunderstorm noise and helicopter noise which contain in the ESC-50 data set on the vehicle audio with the SNR of 10 dB and 5 dB respectively. After extracting features, we put them into the hybrid neural network to train and performed 5 time experiments every kind of noise, then took the average. The comparison experimental are shown in Fig. 11</p>
<p><div align="center"><img src="//chen950203.github.io/2022/06/22/Hybrid/12.png" alt><br></div></p>
<p>Figure 11: The accuracy and loss of experiment.</p>
<p><strong>V Conclusion</strong></p>
<p>In the vehicle identification system, the quality of extracting feature and the performance of the classifier affect the accuracy of recognition. In this paper, we spliced the Mel frequency cepstrum coefficient, the reformative pitch class profile and the short-term energy as the input vector of the hybrid neural networks. The Mel frequency cepstrum coefficient can reflect the auditory properties of human ear. The reformative pitch class profile can reflect the tone of the vehicle audio. When the background noise is low, the short-term energy can reflect the audio characteristics very well. Compared with other identification system for identifying the vehicle type, the developed system in this paper has many advantages. Compared with the support vector machine, for the extraction of different features of audio, in general, the hybrid neural network classifier has a higher recognition effect. Compared with the other features, the novel features can reflect the characteristic of vehicle audio very well, the recognition rate can reach 100% on the data of an extensive real word experiment. The recognition system we proposed can make recognition rate to 98% when the vehicle audio is collected by the microphone. When we destroy the training set randomly, the accuracy of this system is higher, so destroying the training set randomly can greatly improve the anti-noise ability. We proposed the hybrid neural network combining the convolutional neural network with the LSTM unit, which has better fit to time series of vehicle audio and can improve the vehicle identification accuracy significantly. Experimental results that the LSTM unit can improve the recognition accuracy by 3.39%.<br>In future, we would like to collect more vehicle audio and perform the experiment with the new dataset to confirm if the identification system has more advantages than other system.<br><br><strong>Funding Statement:</strong> This research was sponsored by Shanxi “1331 Project” Key Subject Construction.<br><br><strong>Conflicts of Interest:</strong> The authors declare that they have no conflicts of interest to report regarding the present study.<br><strong>References</strong></p>
<ol>
<li>Krishnaswamy Rangarajan, A.; Purushothaman, R., Disease Classification in Eggplant Using Pre-trained VGG16 and MSVM. %J Scientific Reports. 2020, (No.1), 1-11.<br></li>
<li>Lee, B.; Cho, K.-H., Brain-inspired speech segmentation for automatic speech recognition using the speech envelope as a temporal reference. %J Scientific reports. 2016, 37647.<br></li>
<li>Thompson, J.; Hu, J.; Mudaranthakam, D. P.; Streeter, D.; Neums, L.; Park, M.; Koestler, D. C.; Gajewski, B.; Jensen, R.; Mayo, M. S., Relevant Word Order Vectorization for Improved Natural Language Processing in Electronic Health Records %J Scientific Reports. 2019.<br></li>
<li>Chang, B., Performance evaluation and prediction of rudders based on machine learning technology %J Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering. 2019, (No.15), 5746-5757.<br></li>
<li>Jin, M.; Deng, W., Predication of different stages of Alzheimer’s disease using neighborhood component analysis and ensemble decision tree(Article) %J Journal of Neuroscience Methods. 2018, 35-41.<br></li>
<li>Parmley, K. A.; Higgins, R. H.; Ganapathysubramanian, B.; Sarkar, S.; Singh, A. K., Machine Learning Approach for Prescriptive Plant Breeding %J Scientific Reports. 2020.<br></li>
<li>Razaghi-Moghadam, Z.; Nikoloski, Z., Supervised Learning of Gene Regulatory Networks. %J Current protocols in plant biology. 2020, (No.2), e20106.<br></li>
<li>Pagan, D. C.; Phan, T. Q.; Weaver, J. S.; Benson, A. R.; Beaudoin, A. J., 4, Unsupervised learning of dislocation motion. %J Acta Materialia. 2019, 510-518.<br></li>
<li>Engelen, J. E. v.; Hoos, H. H., A survey on semi-supervised learning %J Machine Learning. 2019.<br></li>
<li>Sheth, A.; Gaur, M.; Kursuncu, U.; Wickramarachchi, R., Shades of Knowledge-Infused Learning for Enhancing Deep Learning %J IEEE Internet Computing. 2019, (No.6), 54-63.<br></li>
<li>Wang1, Z.; Zhang1, X.; Yang1, X.; Xia1, W., Robust Vehicle Detection on Multi- Resolution Aerial Images %J IOP Conference Series: Materials Science and Engineering. 2020, (Conference 1), 012064.<br></li>
<li>Hassaballah, M. a.; Kenk, M. A. b.; El-Henawy, I. M. c., Local binary pattern-based on-road vehicle detection in urban traffic scene %J Pattern Analysis and Applications. 2020.<br></li>
<li>Gong-ping1, Y.; Yi-ping1, T.; Wang-ming1, H.; Qi1, C., Vehicle category recognition based on deep convolutional neural network %J Journal of Zhejiang University (Engineering Science). 2018, (No.4), 694-702.<br></li>
<li>Inagaki, K.; Sato, S.; Umezaki, T., A Recurrent Neural Network Approach to Rear Vehicle Detection Which Considered State Dependency %J Journal of Systemics, Cybernetics and Informatics. 2003, (No.4), 72-77.<br></li>
<li>Wang, T.; Zhu, Z., Multimodal and Multi-task Audio-Visual Vehicle Detection and Classification %J 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS). 2012, 440-446.<br></li>
<li>Chen, N.; Downie, J. S.; Zhu, Y.; Xiao, H.-d., Cochlear pitch class profile for cover song identification %J APPLIED ACOUSTICS. 2015, 92-96.<br></li>
<li>Lipton, Z. C.; Berkowitz, J.; Elkan, C., A Critical Review of Recurrent Neural Networks for Sequence Learning. 2015.<br></li>
</ol>
<p>更新至2022.06.23</p>
</div>
            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-chen950203-github-io'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://chen950203.github.io/2022/06/22/Hybrid/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://chen950203.github.io/2022/06/22/Hybrid/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-chen950203-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a href="http://chen950203.github.io " style="border-bottom: none;">陈昊泽</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
